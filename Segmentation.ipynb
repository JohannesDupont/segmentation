{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from models import Unet2D, Unet3D, StandardUnet3D\n",
    "import dataio\n",
    "import utils\n",
    "import models\n",
    "import ants\n",
    "from dataio import *\n",
    "from utils import *\n",
    "from typing import Any\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "K.clear_session()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "importlib.reload(dataio)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(models)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras_radam.optimizer_v2 import RAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'SegmentGSR'\n",
    "# The csv_file variable contains name of CSV file used to load the images\n",
    "csv_file = 'GSR_fucts.csv'\n",
    "# Reduce images to this size\n",
    "new_shape = (256,256,32)\n",
    "# if first coordinate of patch_shape is 0, the whole image is used instead of patches\n",
    "patch_shape = (0,256,32)\n",
    "crop = 0\n",
    "# When using patches, sample n_exmaple patches\n",
    "n_examples = 6 \n",
    "alpha = [7e2,7e2,0]\n",
    "sigma = [6,6,1]\n",
    "augment_funcs = [identity, flip_image_horizontally, flip_image_vertically]\n",
    "\n",
    "if patch_shape[0]>0:\n",
    "    type_str = '_patches_'\n",
    "else:\n",
    "    type_str = '_imgs_'\n",
    "    \n",
    "if augment_funcs:\n",
    "    type_str = type_str + 'augm_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = 'iStroke_148_FUCTS_CTAS.csv'\n",
    "img_type = 'FUCT'\n",
    "subject_df = pd.read_csv(csv_file,comment='#').dropna()\n",
    "subject_df.columns.str.match(\"Unnamed\")\n",
    "subject_df.loc[:,~subject_df.columns.str.match(\"Unnamed\")]\n",
    "subject_df = subject_df.query(\"vol > 50000\")\n",
    "strat_col = 'vol'\n",
    "train_df, test_df = split_dataset(subject_df,strat_col,0.1,0)\n",
    "train_df, val_df = split_dataset(train_df,strat_col,0.1,0)\n",
    "\n",
    "file_writer = tf.summary.create_file_writer(prefix+'/logs')\n",
    "\n",
    "train_TFRfile = prefix + '/train' + type_str + '.TFRecords'\n",
    "val_TFRfile = prefix + '/validation' + type_str + '.TFRecords'\n",
    "test_TFRfile = prefix + '/test' + type_str + '.TFRecords'\n",
    "\n",
    "if patch_shape[0]==0:\n",
    "    example_shape = (new_shape[0],new_shape[1],patch_shape[2])\n",
    "else:\n",
    "    example_shape = patch_shape\n",
    "\n",
    "print(subject_df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from icecream import ic\n",
    "plt.hist(train_df['vol'],3)\n",
    "plt.show()\n",
    "# ic(train_df.shape[0])\n",
    "\n",
    "plt.hist(val_df['vol'],3)\n",
    "plt.show()\n",
    "# ic(val_df.shape[0])\n",
    "\n",
    "plt.hist(test_df['vol'],3)\n",
    "plt.show()\n",
    "# ic(test_df.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training data...')\n",
    "train_TFRfile = subjects_to_TFRecords(train_df, img_type, train_TFRfile, patch_shape, new_shape, crop, n_examples, augment_funcs)\n",
    "train_ds = tf.data.TFRecordDataset(train_TFRfile).map(read_tfrecord(example_shape))\n",
    "print(f'Number of subjects in training dataset is {train_df.shape[0]}')\n",
    "print(f'Number of samples in training dataset is {get_dataset_size(train_ds)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('validation data...')\n",
    "val_TFRfile = subjects_to_TFRecords(val_df,img_type, val_TFRfile, patch_shape, new_shape, crop, n_examples, None)\n",
    "val_ds = tf.data.TFRecordDataset(val_TFRfile).map(read_tfrecord(example_shape))\n",
    "print(f'Number of subjects in validation dataset is {val_df.shape[0]}')\n",
    "print(f'Number of samples in validation dataset is {get_dataset_size(val_ds)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testing data...')\n",
    "test_TFRfile = subjects_to_TFRecords(test_df,img_type, test_TFRfile, patch_shape, new_shape, crop, n_examples, None)\n",
    "test_ds = tf.data.TFRecordDataset(test_TFRfile).map(read_tfrecord(example_shape))\n",
    "print(f'Number of subjects in testing dataset is {test_df.shape[0]}')\n",
    "print(f'Number of samples in testing dataset is {get_dataset_size(test_ds)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.TFRecordDataset(train_TFRfile).map(read_tfrecord(example_shape)).map(ds_clip(0,150))\n",
    "val_ds = tf.data.TFRecordDataset(val_TFRfile).map(read_tfrecord(example_shape)).map(ds_clip(0,150))\n",
    "test_ds = tf.data.TFRecordDataset(test_TFRfile).map(read_tfrecord(example_shape)).map(ds_clip(0,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from icecream import ic\n",
    "def plot_patch_label_pair(patch: np.array, label: np.array) -> Any:\n",
    "    slvol = np.sum(label,axis=(0,1))\n",
    "    idx = np.where(slvol==np.max(slvol))[0][0]\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    im = ax1.imshow(np.squeeze(patch[:,:,idx]))\n",
    "    ax1.grid(False)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.imshow(np.squeeze(label[:,:,idx]))\n",
    "    ax2.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "ds = train_ds\n",
    "for p,l in ds.take(6):\n",
    "    print(p.shape)\n",
    "    plot_patch_label_pair(p,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LR Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "class LearningRateDecay:\n",
    "    def plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
    "        # compute the set of learning rates for each corresponding\n",
    "        # epoch\n",
    "        lrs = [self(i) for i in epochs]\n",
    "        # the learning rate schedule\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, lrs)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        \n",
    "class StepDecay(LearningRateDecay):\n",
    "    def __init__(self, initAlpha=0.01, factor=0.1, dropEvery=10):\n",
    "        # store the base initial learning rate, drop factor, and\n",
    "        # epochs to drop every\n",
    "        self.initAlpha = initAlpha\n",
    "        self.factor = factor\n",
    "        self.dropEvery = dropEvery\n",
    "    def __call__(self, epoch):\n",
    "        # compute the learning rate for the current epoch\n",
    "        exp = np.floor((1 + epoch) / self.dropEvery)\n",
    "        alpha = self.initAlpha / (1+ self.factor * epoch)\n",
    "        # return the learning rate\n",
    "        return float(alpha)\n",
    "    \n",
    "step_schedule = StepDecay(initAlpha=1e-3,factor=0.01, dropEvery = 100)\n",
    "step_schedule.plot(range(0,400))\n",
    "lr_callback = LearningRateScheduler(step_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "file_writer = tf.summary.create_file_writer(prefix+'/logs')\n",
    "dtnow=datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "log_dir=prefix+\"/logs/logs_\" + dtnow\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0,profile_batch=0,embeddings_freq=0)\n",
    "\n",
    "# Set up checkpoints \n",
    "checkpoint_path = prefix+ \"/checkpoints/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                             save_weights_only=True,save_best_only = True,\n",
    "                                             verbose=1,monitor='val_dice_coef',mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import dice_coef_loss, dice_coef\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "conf = dict(model = 'UNET',\n",
    "            filters = [16,32,64],\n",
    "            bnorm=True,\n",
    "            SN = False,\n",
    "            se = False,\n",
    "            SA = False,\n",
    "            r = 8,\n",
    "            alpha = 0.0,\n",
    "            dropout = 0.0,\n",
    "            lr = 1e-3,\n",
    "            wd=0.0e-4,\n",
    "            spatial_dropout = 0.20,\n",
    "            ratio=2,\n",
    "            bs=1,\n",
    "            epochs=300,\n",
    "            noise_sigma=0,\n",
    "            init = 'lecun_uniform'\n",
    "            )\n",
    "print(conf)\n",
    "\n",
    "if patch_shape[-1] > 1:\n",
    "    input_img = Input(shape=(None, None,None, 1))\n",
    "    model = Unet3D(input_img,conf)\n",
    "else:\n",
    "    input_img = Input(shape=(None, None, 1))\n",
    "    model = Unet2D(input_img,conf)\n",
    "    \n",
    "Optimizer = Adam(conf['lr'])\n",
    "loss = dice_coef_loss\n",
    "miou = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "model.compile(optimizer=Optimizer,loss=loss, metrics=['accuracy', miou,dice_coef])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Fitting model over {conf[\"epochs\"]} epochs with batch size of {conf[\"bs\"]} ....')\n",
    "tf.keras.backend.clear_session()\n",
    "history = model.fit(train_ds.batch(conf[\"bs\"],drop_remainder=True),validation_data=val_ds.batch(1),\\\n",
    "                            epochs=conf['epochs'],callbacks=[tensorboard_callback,cp_callback],verbose=0)#,callbacks=[cp_callback])# callbacks=[tensorboard_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "model_path = prefix + '/GSR_CTAPRED_256x256x32_0.h5'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds.batch(1),batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_type)\n",
    "results = predict_dataframe(model,test_df, img_type, example_shape, new_shape, crop,(0,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.query(\"vol_gt>2500\")\n",
    "print(df)\n",
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= predict_subject([model_path],test_df.iloc[0],img_type,new_shape,example_shape, crop,True,False,(0,150),16,True,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
